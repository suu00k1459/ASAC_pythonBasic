{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DB9BKWVa-IBr"
      },
      "outputs": [],
      "source": [
        "# GPU는 무조건 T4라도 연결하고 사용하셔야 합니다!!!!!!!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "7zjze5dd-PAy"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a5YuYZIofJOz"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "(train_X, train_y),(test_X, test_y) = fashion_mnist.load_data()\n",
        "print(train_X.shape)\n",
        "print(train_y.shape)\n",
        "print(test_X.shape)\n",
        "print(test_y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOePQ6MqCpWZ",
        "outputId": "2cffa963-0c54-43f5-def9-126656bdc0ed"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 0s 0us/step\n",
            "(60000, 28, 28)\n",
            "(60000,)\n",
            "(10000, 28, 28)\n",
            "(10000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 전처리 : 이미지 전처리 --> 1/255.0으로 정규화\n",
        "train_X = train_X/255.0\n",
        "test_X = test_X /255.0"
      ],
      "metadata": {
        "id": "NQ3aA_cpCpZH"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-gh3NcoD9Zb",
        "outputId": "2a917ac9-106f-49e8-a608-6ecbf4649512"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_X[0].shape # 2d"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7XurHFoEJ_V",
        "outputId": "34d11a70-3400-4cb3-e1df-e57e194e2a82"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_X.shape)\n",
        "print(train_X[0].shape)\n",
        "train_X = train_X.reshape(-1 ,28,28,1)  # 3d ---> 4d\n",
        "test_X  = test_X.reshape(-1, 28,28,1)\n",
        "print(train_X.shape)\n",
        "print(train_X[0].shape)\n",
        "# 목적 : 모델의 구조도에 맞춰서 데이터셋을 변형!!!!!"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vdnEllMEcAq",
        "outputId": "ea9db8ee-84e3-4f41-ceb9-22afaa1bde4a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 28, 28)\n",
            "(28, 28)\n",
            "(60000, 28, 28, 1)\n",
            "(28, 28, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### VGG 선행 연구 결과를 참조"
      ],
      "metadata": {
        "id": "fM4jxUhd1DQk"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://neurohive.io/wp-content/uploads/2018/11/vgg16.png\">"
      ],
      "metadata": {
        "id": "Oyogykjw1FEA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "https://arxiv.org/pdf/1409.1556"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "cbABlOqI1MA4",
        "outputId": "a70007c1-0098-49bc-ef66-fbf5ba7f8535"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-12-d63fc2938d6b>, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-12-d63fc2938d6b>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    https://arxiv.org/pdf/1409.1556\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# - 기존의 설계 방식에서는 Conv - Pool 구조를 이어서 사용\n",
        "#   ==> Conv-Conv-Pool/ Conv-Conv-Conv-Pool\n",
        "# 기존에는 대략 큰 필터들을 사용을 했음 7*7, 9*9, 11*11 을 사용했음\n",
        "\n",
        "# 10*10에서 7*7(필터) 하면 49C2 연산이 필요, 결과는 4*4\n",
        "# 10*10에서 3*3(필터) 하면 9C2 연산이 필요, 결과는 4*4\n",
        "# 결과물이 똑같지만 연산량이 확 줄기 때문에 적층을 할 수 있음\n",
        "# + AF 집어넣은 것들은 1번 보다 3번 통과해서 더 비선형을 획득할 수 있음"
      ],
      "metadata": {
        "id": "-Zq7yS2g1c1C"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 논문에서 연산도 줄고, 잘 된다고 하고, 내가 원하면 더 층을 쌓을 수도 있는\n",
        "# 응용부분도 존재 : 튜닝이 쉬움\n",
        "# 기본 구조 : 늘 했던 단순 적층 구조\n"
      ],
      "metadata": {
        "id": "VZWMPzWm1cv2"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 코드가 길어지니 간략하게 불러오겠음\n",
        "from tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, Dense, Dropout"
      ],
      "metadata": {
        "id": "ocBgFy2Z1cpQ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 기본적으로 VGG 설계 방식 차용\n",
        "# -> 입력 데이터가 다르기 때문\n",
        "# 논문 상 입력 : 224, 224, 3\n",
        "# 나의 데이터 :28, 28, 1\n",
        "# sol1) 논문상 데이터 크기 맞추기\n",
        "# sol2) 이해했을 경우, 상화엥 맞춰 설계"
      ],
      "metadata": {
        "id": "lLiXP8kL1cdX"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "F6tz_PKH75jk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_vgg_fashion = tf.keras.Sequential(\n",
        "    [\n",
        "        # 원본 입력을 기준으로 설계.... : 224 224 RGB\n",
        "        # --> 28 28 1\n",
        "        Conv2D( input_shape=(28,28,1), kernel_size=(3,3),\n",
        "               filters=64, padding =\"same\", activation=\"relu\"),\n",
        "        Conv2D(  kernel_size=(3,3),\n",
        "               filters=64, padding =\"same\", activation=\"relu\"),\n",
        "        MaxPool2D( pool_size=(2,2), strides=(2,2)),\n",
        "\n",
        "        Conv2D(  kernel_size=(3,3),\n",
        "               filters=128, padding =\"same\", activation=\"relu\"),\n",
        "        Conv2D(  kernel_size=(3,3),\n",
        "               filters=128, padding =\"same\", activation=\"relu\"),\n",
        "        MaxPool2D( pool_size=(2,2), strides=(2,2)),\n",
        "\n",
        "        Conv2D(  kernel_size=(3,3),\n",
        "               filters=256, padding =\"same\", activation=\"relu\"),\n",
        "        Conv2D(  kernel_size=(3,3),\n",
        "               filters=256, padding =\"same\", activation=\"relu\"),\n",
        "        Conv2D(  kernel_size=(3,3),\n",
        "               filters=256, padding =\"same\", activation=\"relu\"),\n",
        "        MaxPool2D( pool_size=(2,2), strides=(2,2)),\n",
        "\n",
        "        Conv2D(  kernel_size=(3,3),\n",
        "               filters=256, padding =\"same\", activation=\"relu\"),\n",
        "        Conv2D(  kernel_size=(3,3),\n",
        "               filters=256, padding =\"same\", activation=\"relu\"),\n",
        "        Conv2D(  kernel_size=(3,3),\n",
        "               filters=256, padding =\"same\", activation=\"relu\"),\n",
        "        MaxPool2D( pool_size=(2,2), strides=(2,2)),\n",
        "\n",
        "        # Conv2D(  kernel_size=(3,3),\n",
        "        #        filters=512, padding =\"same\", activation=\"relu\"),\n",
        "        # Conv2D(  kernel_size=(3,3),\n",
        "        #        filters=512, padding =\"same\", activation=\"relu\"),\n",
        "        # Conv2D(  kernel_size=(3,3),\n",
        "        #        filters=512, padding =\"same\", activation=\"relu\"),\n",
        "        # MaxPool2D( pool_size=(2,2), strides=(2,2)),\n",
        "\n",
        "        # Conv2D(  kernel_size=(3,3),\n",
        "        #        filters=512, padding =\"same\", activation=\"relu\"),\n",
        "        # Conv2D(  kernel_size=(3,3),\n",
        "        #        filters=512, padding =\"same\", activation=\"relu\"),\n",
        "        # Conv2D(  kernel_size=(3,3),\n",
        "        #        filters=512, padding =\"same\", activation=\"relu\"),\n",
        "        # MaxPool2D( pool_size=(2,2), strides=(2,2)),\n",
        "        # ===> 기본적인 이미지가 가지고 있는 특징 추출!!!!!\n",
        "\n",
        "        # 분류를 위한 NN\n",
        "        # 1) 특징을 분류 Faltten\n",
        "        Flatten(),\n",
        "        # 2) 분류용 HL 적층!!!!\n",
        "        Dense( units=4096, activation=\"relu\"),\n",
        "        Dense( units=4096, activation=\"relu\"),\n",
        "        # 3) 출력용 --> 내 모델의 목적에 맞춰야함!!\n",
        "        #    나는 10개만 하면 됨...수정..\n",
        "        Dense( units=10, activation=\"softmax\")\n",
        "    ]\n",
        ")\n",
        "model_vgg_fashion"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iKgDaBAQ6c55",
        "outputId": "7db4345c-bdb0-4885-9691-82b3eaf1927b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.engine.sequential.Sequential at 0x7ec45016b5b0>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "pUkI7fOCfZRV"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --> epochs를 일단 크게 세팅을 하고,,,\n",
        "#     early stop을 통해서 더이상 성능 향상이 없으면 stop\n",
        "# --> 중간 중간 모델의 weights를 저장(모델 저장)\n",
        "cp_path = \"training/cp-{epoch:04d}.ckpt\"\n",
        "cp_dir = os.path.dirname(cp_path) # training\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    cp_path,\n",
        "    verbose = 1,\n",
        "    save_weights_only = True\n",
        ")\n",
        "es_callback = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor=\"val_loss\",\n",
        "    patience = 10 # 참을 횟수...갱신이 안 되는 횟수..\n",
        ")\n",
        "\n",
        "model_vgg_fashion.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(),\n",
        "    loss = \"sparse_categorical_crossentropy\",\n",
        "    metrics =[\"accuracy\"]\n",
        ")\n",
        "\n",
        "history = model_vgg_fashion.fit( train_X, train_y,\n",
        "                    epochs = 200,\n",
        "                     validation_split = 0.25,\n",
        "                     batch_size=128,\n",
        "                     callbacks=[cp_callback,es_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjFMRHUK_ZDA",
        "outputId": "508ebf51-8c8c-41cb-bf92-bddd77482361"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.7661 - accuracy: 0.6984\n",
            "Epoch 1: saving model to training/cp-0001.ckpt\n",
            "352/352 [==============================] - 22s 50ms/step - loss: 0.7661 - accuracy: 0.6984 - val_loss: 0.4262 - val_accuracy: 0.8440\n",
            "Epoch 2/200\n",
            "351/352 [============================>.] - ETA: 0s - loss: 0.3428 - accuracy: 0.8753\n",
            "Epoch 2: saving model to training/cp-0002.ckpt\n",
            "352/352 [==============================] - 16s 44ms/step - loss: 0.3429 - accuracy: 0.8753 - val_loss: 0.3160 - val_accuracy: 0.8857\n",
            "Epoch 3/200\n",
            "351/352 [============================>.] - ETA: 0s - loss: 0.2832 - accuracy: 0.8978\n",
            "Epoch 3: saving model to training/cp-0003.ckpt\n",
            "352/352 [==============================] - 15s 44ms/step - loss: 0.2832 - accuracy: 0.8977 - val_loss: 0.2889 - val_accuracy: 0.8978\n",
            "Epoch 4/200\n",
            "351/352 [============================>.] - ETA: 0s - loss: 0.2515 - accuracy: 0.9081\n",
            "Epoch 4: saving model to training/cp-0004.ckpt\n",
            "352/352 [==============================] - 15s 43ms/step - loss: 0.2515 - accuracy: 0.9081 - val_loss: 0.2762 - val_accuracy: 0.8989\n",
            "Epoch 5/200\n",
            "351/352 [============================>.] - ETA: 0s - loss: 0.2329 - accuracy: 0.9172\n",
            "Epoch 5: saving model to training/cp-0005.ckpt\n",
            "352/352 [==============================] - 16s 44ms/step - loss: 0.2329 - accuracy: 0.9172 - val_loss: 0.2419 - val_accuracy: 0.9125\n",
            "Epoch 6/200\n",
            "351/352 [============================>.] - ETA: 0s - loss: 0.2059 - accuracy: 0.9253\n",
            "Epoch 6: saving model to training/cp-0006.ckpt\n",
            "352/352 [==============================] - 16s 45ms/step - loss: 0.2057 - accuracy: 0.9254 - val_loss: 0.2491 - val_accuracy: 0.9129\n",
            "Epoch 7/200\n",
            "351/352 [============================>.] - ETA: 0s - loss: 0.1873 - accuracy: 0.9327\n",
            "Epoch 7: saving model to training/cp-0007.ckpt\n",
            "352/352 [==============================] - 15s 44ms/step - loss: 0.1872 - accuracy: 0.9327 - val_loss: 0.2389 - val_accuracy: 0.9153\n",
            "Epoch 8/200\n",
            "351/352 [============================>.] - ETA: 0s - loss: 0.1716 - accuracy: 0.9381\n",
            "Epoch 8: saving model to training/cp-0008.ckpt\n",
            "352/352 [==============================] - 15s 43ms/step - loss: 0.1716 - accuracy: 0.9380 - val_loss: 0.2344 - val_accuracy: 0.9231\n",
            "Epoch 9/200\n",
            "351/352 [============================>.] - ETA: 0s - loss: 0.1516 - accuracy: 0.9446\n",
            "Epoch 9: saving model to training/cp-0009.ckpt\n",
            "352/352 [==============================] - 17s 47ms/step - loss: 0.1515 - accuracy: 0.9446 - val_loss: 0.2897 - val_accuracy: 0.9068\n",
            "Epoch 10/200\n",
            "351/352 [============================>.] - ETA: 0s - loss: 0.1472 - accuracy: 0.9470\n",
            "Epoch 10: saving model to training/cp-0010.ckpt\n",
            "352/352 [==============================] - 21s 59ms/step - loss: 0.1472 - accuracy: 0.9469 - val_loss: 0.2738 - val_accuracy: 0.9072\n",
            "Epoch 11/200\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1317 - accuracy: 0.9535\n",
            "Epoch 11: saving model to training/cp-0011.ckpt\n",
            "352/352 [==============================] - 22s 64ms/step - loss: 0.1317 - accuracy: 0.9535 - val_loss: 0.2526 - val_accuracy: 0.9229\n",
            "Epoch 12/200\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1189 - accuracy: 0.9581\n",
            "Epoch 12: saving model to training/cp-0012.ckpt\n",
            "352/352 [==============================] - 16s 45ms/step - loss: 0.1189 - accuracy: 0.9581 - val_loss: 0.2518 - val_accuracy: 0.9227\n",
            "Epoch 13/200\n",
            "352/352 [==============================] - ETA: 0s - loss: 0.1074 - accuracy: 0.9616\n",
            "Epoch 13: saving model to training/cp-0013.ckpt\n",
            "352/352 [==============================] - 17s 49ms/step - loss: 0.1074 - accuracy: 0.9616 - val_loss: 0.2471 - val_accuracy: 0.9281\n",
            "Epoch 14/200\n",
            "351/352 [============================>.] - ETA: 0s - loss: 0.0945 - accuracy: 0.9665\n",
            "Epoch 14: saving model to training/cp-0014.ckpt\n",
            "352/352 [==============================] - 23s 66ms/step - loss: 0.0944 - accuracy: 0.9665 - val_loss: 0.2781 - val_accuracy: 0.9227\n",
            "Epoch 15/200\n",
            "351/352 [============================>.] - ETA: 0s - loss: 0.0933 - accuracy: 0.9690\n",
            "Epoch 15: saving model to training/cp-0015.ckpt\n",
            "352/352 [==============================] - 15s 43ms/step - loss: 0.0932 - accuracy: 0.9690 - val_loss: 0.2846 - val_accuracy: 0.9205\n",
            "Epoch 16/200\n",
            "351/352 [============================>.] - ETA: 0s - loss: 0.0897 - accuracy: 0.9690\n",
            "Epoch 16: saving model to training/cp-0016.ckpt\n",
            "352/352 [==============================] - 16s 45ms/step - loss: 0.0897 - accuracy: 0.9690 - val_loss: 0.3154 - val_accuracy: 0.9203\n",
            "Epoch 17/200\n",
            "351/352 [============================>.] - ETA: 0s - loss: 0.0755 - accuracy: 0.9743\n",
            "Epoch 17: saving model to training/cp-0017.ckpt\n",
            "352/352 [==============================] - 15s 43ms/step - loss: 0.0756 - accuracy: 0.9743 - val_loss: 0.3110 - val_accuracy: 0.9181\n",
            "Epoch 18/200\n",
            "351/352 [============================>.] - ETA: 0s - loss: 0.0695 - accuracy: 0.9755\n",
            "Epoch 18: saving model to training/cp-0018.ckpt\n",
            "352/352 [==============================] - 15s 43ms/step - loss: 0.0694 - accuracy: 0.9756 - val_loss: 0.3841 - val_accuracy: 0.9215\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_vgg_fashion.evaluate(test_X, test_y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SkgKT6cFFe1J",
        "outputId": "7bc78b39-71a3-4ce3-b134-2607bf82c038"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 2s 6ms/step - loss: 0.3862 - accuracy: 0.9230\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.386211633682251, 0.9229999780654907]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ++conv 필터 사이즈, conv 필터 개수, conv layer 깊이\n",
        "# 분류에 있는 dense 깊이, 노드, ac\n",
        "# ++ dropout\n",
        "# 실험해보기"
      ],
      "metadata": {
        "id": "-TmuaQRYF8RM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cnn 에서 이미지 분류 쪽에서 주로 개선방향\n",
        "# 1) 최신 모델 구조 차용\n",
        "# 2) 데이터 수가 모델의 파라미터 수 대비 부족\n",
        "# => 가상의 데이터를 생성해서 이미지를 보충, 입력 데이터의 샘플 수를 엄청 늘리는 직업\n",
        "# agumentation 이미지 증강, 보강 -> 성능 상승!"
      ],
      "metadata": {
        "id": "KeXcfxElGOOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# try1) 모델의 구조\n",
        "# try2) 데이터의 수! + 직접 수집을 더 하면 됨\n",
        "# -> 수집한 데이터를 기반으로 가상의 이미지를 생성\n",
        "# (bagging -> bootstraping 유사한 데이터셋을 여러개 생성!)\n",
        "# 원본과 거의 유사하지만 다른 가상의 데이터 생성 (이미지 보강, 증강)\n",
        "\n",
        "# 활용방법1) 직접 이미지를 쌓아서 처리 : (60000,28,28,1)->(90000,28,28,1)\n",
        "#           입력 데이터가 그대로 존재해서 코드상 손댈 부분. 굳이 HDD 차지할 필요가 있을까?\n",
        "# 활용방법2) 학습할 때만 생성을 하고, 학습 후에는 제거!\n",
        "#           -> 원천 소스가 되는 이미지느 그대로 두고, 파생시킨 이미지들은"
      ],
      "metadata": {
        "id": "fZAU6ziNhV_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rotation_range = 10\n",
        "\n",
        "img_gen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    rotation_range = 10, # 단, 너무 상하 반전이 이미지도 생성이 될 수 있음\n",
        "    zoom_range=0.10, # 너무 커서 넘어가거나, 너무 작아서 안보이거나\n",
        "    horizontal_flip = True,\n",
        "    vertical_flip = False,\n",
        "    # etc\n",
        "\n",
        ")\n",
        "\n",
        "# https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator"
      ],
      "metadata": {
        "id": "OkU8yiPDjMir"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 참고) 직접 만들어서 추가하는 것은 생략 (거의 사용 안함)\n",
        "# img_gen.flow~~"
      ],
      "metadata": {
        "id": "Lr3jNU3-kGyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 위의 generator를 기반으로 hdd는 하지 않지만,\n",
        "# 만들 수 있는 generator에 대한 세팅!!!!!!\n",
        "# ==> batch단위로 처리를 함!!!!\n",
        "# ( 폴더 중심으로 세팅을 하는게 일반적인데,,,,주어진 셋으로 )\n",
        "# ==> yolo 모델을 가지고 할 때는 폴더 중심으로 하겠습니다.."
      ],
      "metadata": {
        "id": "EkbL_yzEk2Mb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터에 대한 셋도 미리 만들어 둬야 함\n",
        "# 핵심은 직접 train/val 생성을 통해서 만들어 주어야 함\n",
        "\n",
        "train_batches = img_gen.flow(train_X, train_y, batch_size = 256)\n",
        "val_batches = img_gen.flow(test_X, test_y, batch_size = 256)\n",
        "\n",
        "# --> val에 평가하는 이미지가 난이도가 높아짐\n",
        "# --> 각각 나눠서 생성을 하는 경우가 있음.(지금은 그냥 코드가 길어지니깐 한번에 처리)"
      ],
      "metadata": {
        "id": "Xygw5T3Pk_LE"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_vgg_fashion = tf.keras.Sequential(\n",
        "    [\n",
        "        # 원본 입력을 기준으로 설계.... : 224 224 RGB\n",
        "        # --> 28 28 1\n",
        "        Conv2D( input_shape=(28,28,1), kernel_size=(3,3),\n",
        "               filters=64, padding =\"same\", activation=\"relu\"),\n",
        "        Conv2D(  kernel_size=(3,3),\n",
        "               filters=64, padding =\"same\", activation=\"relu\"),\n",
        "        MaxPool2D( pool_size=(2,2), strides=(2,2)),\n",
        "\n",
        "        Conv2D(  kernel_size=(3,3),\n",
        "               filters=128, padding =\"same\", activation=\"relu\"),\n",
        "        Conv2D(  kernel_size=(3,3),\n",
        "               filters=128, padding =\"same\", activation=\"relu\"),\n",
        "        MaxPool2D( pool_size=(2,2), strides=(2,2)),\n",
        "\n",
        "        Conv2D(  kernel_size=(3,3),\n",
        "               filters=256, padding =\"same\", activation=\"relu\"),\n",
        "        Conv2D(  kernel_size=(3,3),\n",
        "               filters=256, padding =\"same\", activation=\"relu\"),\n",
        "        Conv2D(  kernel_size=(3,3),\n",
        "               filters=256, padding =\"same\", activation=\"relu\"),\n",
        "        MaxPool2D( pool_size=(2,2), strides=(2,2)),\n",
        "\n",
        "        # Conv2D(  kernel_size=(3,3),\n",
        "        #        filters=512, padding =\"same\", activation=\"relu\"),\n",
        "        # Conv2D(  kernel_size=(3,3),\n",
        "        #        filters=512, padding =\"same\", activation=\"relu\"),\n",
        "        # Conv2D(  kernel_size=(3,3),\n",
        "        #        filters=512, padding =\"same\", activation=\"relu\"),\n",
        "        # MaxPool2D( pool_size=(2,2), strides=(2,2)),\n",
        "\n",
        "        # Conv2D(  kernel_size=(3,3),\n",
        "        #        filters=512, padding =\"same\", activation=\"relu\"),\n",
        "        # Conv2D(  kernel_size=(3,3),\n",
        "        #        filters=512, padding =\"same\", activation=\"relu\"),\n",
        "        # Conv2D(  kernel_size=(3,3),\n",
        "        #        filters=512, padding =\"same\", activation=\"relu\"),\n",
        "        # MaxPool2D( pool_size=(2,2), strides=(2,2)),\n",
        "        # ===> 기본적인 이미지가 가지고 있는 특징 추출!!!!!\n",
        "\n",
        "        # 분류를 위한 NN\n",
        "        # 1) 특징을 분류 Faltten\n",
        "        Flatten(),\n",
        "        # 2) 분류용 HL 적층!!!!\n",
        "        Dense( units=4096, activation=\"relu\"),\n",
        "        Dense( units=4096, activation=\"relu\"),\n",
        "        # 3) 출력용 --> 내 모델의 목적에 맞춰야함!!\n",
        "        #    나는 10개만 하면 됨...수정..\n",
        "        Dense( units=10, activation=\"softmax\")\n",
        "    ]\n",
        ")\n",
        "model_vgg_fashion"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2NttnQclJBC",
        "outputId": "caa67ec0-8a0b-4c76-ce66-7d7b049bb415"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.engine.sequential.Sequential at 0x7ec3e47de7d0>"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --> epochs를 일단 크게 세팅을 하고,,,\n",
        "#     early stop을 통해서 더이상 성능 향상이 없으면 stop\n",
        "# --> 중간 중간 모델의 weights를 저장(모델 저장)\n",
        "cp_path = \"training/cp-{epoch:04d}.ckpt\"\n",
        "cp_dir = os.path.dirname(cp_path) # training\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    cp_path,\n",
        "    verbose = 1,\n",
        "    save_weights_only = True\n",
        ")\n",
        "es_callback = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor=\"val_loss\",\n",
        "    patience = 10 # 참을 횟수...갱신이 안 되는 횟수..\n",
        ")\n",
        "\n",
        "model_vgg_fashion.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(),\n",
        "    loss = \"sparse_categorical_crossentropy\",\n",
        "    metrics =[\"accuracy\"]\n",
        ")\n",
        "\n",
        "# 검색했을 때 fit_generator : 이미지 증강\n",
        "# --> fit을 통합할 거임\n",
        "\n",
        "# pytorch 를 무조건 데이셋을 중심으로 핸들링을 함!!!\n",
        "# ==> 생성기를 통해서 처리하는 코드 스타일이 일반적!!!\n",
        "# pytorch로 실습할 때 설명을 드리겠습니다..class + decortor + template etc\n",
        "# 프로그램으로 -p.py중심으로 구성을 해야함!!!\n",
        "# 모듈화!!!!\n",
        "\n",
        "# fit으로 통합해서 혼동 발생 가능\n",
        "# X, y 각각 넣어서 학습했는지, 덩어리로 처리했는지에 따라\n",
        "# 파라미터 옵션이 달라지니 확인해야함.\n",
        "\n",
        "history = model_vgg_fashion.fit(train_batches, # 성기를 중심으로 셋.셋 생성기\n",
        "                    epochs = 200,\n",
        "                    validation_data = val_batches,\n",
        "                    callbacks=[cp_callback,es_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RXf9ulXWmYEF",
        "outputId": "0bd0ce20-7ace-44fe-8d65-523b99f5d3ff"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "235/235 [==============================] - ETA: 0s - loss: 0.7590 - accuracy: 0.7115\n",
            "Epoch 1: saving model to training/cp-0001.ckpt\n",
            "235/235 [==============================] - 37s 131ms/step - loss: 0.7590 - accuracy: 0.7115 - val_loss: 0.4546 - val_accuracy: 0.8300\n",
            "Epoch 2/200\n",
            "235/235 [==============================] - ETA: 0s - loss: 0.3914 - accuracy: 0.8509\n",
            "Epoch 2: saving model to training/cp-0002.ckpt\n",
            "235/235 [==============================] - 23s 99ms/step - loss: 0.3914 - accuracy: 0.8509 - val_loss: 0.3587 - val_accuracy: 0.8633\n",
            "Epoch 3/200\n",
            "235/235 [==============================] - ETA: 0s - loss: 0.3150 - accuracy: 0.8824\n",
            "Epoch 3: saving model to training/cp-0003.ckpt\n",
            "235/235 [==============================] - 24s 100ms/step - loss: 0.3150 - accuracy: 0.8824 - val_loss: 0.3363 - val_accuracy: 0.8835\n",
            "Epoch 4/200\n",
            "235/235 [==============================] - ETA: 0s - loss: 0.2779 - accuracy: 0.8959\n",
            "Epoch 4: saving model to training/cp-0004.ckpt\n",
            "235/235 [==============================] - 22s 95ms/step - loss: 0.2779 - accuracy: 0.8959 - val_loss: 0.2928 - val_accuracy: 0.8932\n",
            "Epoch 5/200\n",
            "235/235 [==============================] - ETA: 0s - loss: 0.2623 - accuracy: 0.9029\n",
            "Epoch 5: saving model to training/cp-0005.ckpt\n",
            "235/235 [==============================] - 29s 124ms/step - loss: 0.2623 - accuracy: 0.9029 - val_loss: 0.2751 - val_accuracy: 0.8964\n",
            "Epoch 6/200\n",
            "235/235 [==============================] - ETA: 0s - loss: 0.2437 - accuracy: 0.9086\n",
            "Epoch 6: saving model to training/cp-0006.ckpt\n",
            "235/235 [==============================] - 23s 97ms/step - loss: 0.2437 - accuracy: 0.9086 - val_loss: 0.2642 - val_accuracy: 0.9032\n",
            "Epoch 7/200\n",
            "235/235 [==============================] - ETA: 0s - loss: 0.2352 - accuracy: 0.9132\n",
            "Epoch 7: saving model to training/cp-0007.ckpt\n",
            "235/235 [==============================] - 29s 123ms/step - loss: 0.2352 - accuracy: 0.9132 - val_loss: 0.2540 - val_accuracy: 0.9050\n",
            "Epoch 8/200\n",
            "235/235 [==============================] - ETA: 0s - loss: 0.2204 - accuracy: 0.9185\n",
            "Epoch 8: saving model to training/cp-0008.ckpt\n",
            "235/235 [==============================] - 28s 121ms/step - loss: 0.2204 - accuracy: 0.9185 - val_loss: 0.2422 - val_accuracy: 0.9135\n",
            "Epoch 9/200\n",
            "235/235 [==============================] - ETA: 0s - loss: 0.2154 - accuracy: 0.9199\n",
            "Epoch 9: saving model to training/cp-0009.ckpt\n",
            "235/235 [==============================] - 23s 99ms/step - loss: 0.2154 - accuracy: 0.9199 - val_loss: 0.2409 - val_accuracy: 0.9122\n",
            "Epoch 10/200\n",
            "235/235 [==============================] - ETA: 0s - loss: 0.2013 - accuracy: 0.9250\n",
            "Epoch 10: saving model to training/cp-0010.ckpt\n",
            "235/235 [==============================] - 24s 100ms/step - loss: 0.2013 - accuracy: 0.9250 - val_loss: 0.2529 - val_accuracy: 0.9075\n",
            "Epoch 11/200\n",
            "235/235 [==============================] - ETA: 0s - loss: 0.1959 - accuracy: 0.9254\n",
            "Epoch 11: saving model to training/cp-0011.ckpt\n",
            "235/235 [==============================] - 24s 100ms/step - loss: 0.1959 - accuracy: 0.9254 - val_loss: 0.2320 - val_accuracy: 0.9160\n",
            "Epoch 12/200\n",
            "235/235 [==============================] - ETA: 0s - loss: 0.1900 - accuracy: 0.9291\n",
            "Epoch 12: saving model to training/cp-0012.ckpt\n",
            "235/235 [==============================] - 25s 107ms/step - loss: 0.1900 - accuracy: 0.9291 - val_loss: 0.2307 - val_accuracy: 0.9164\n",
            "Epoch 13/200\n",
            "235/235 [==============================] - ETA: 0s - loss: 0.1808 - accuracy: 0.9323\n",
            "Epoch 13: saving model to training/cp-0013.ckpt\n",
            "235/235 [==============================] - 23s 99ms/step - loss: 0.1808 - accuracy: 0.9323 - val_loss: 0.2274 - val_accuracy: 0.9176\n",
            "Epoch 14/200\n",
            "235/235 [==============================] - ETA: 0s - loss: 0.1758 - accuracy: 0.9341\n",
            "Epoch 14: saving model to training/cp-0014.ckpt\n",
            "235/235 [==============================] - 23s 97ms/step - loss: 0.1758 - accuracy: 0.9341 - val_loss: 0.2322 - val_accuracy: 0.9193\n",
            "Epoch 15/200\n",
            "235/235 [==============================] - ETA: 0s - loss: 0.1722 - accuracy: 0.9349\n",
            "Epoch 15: saving model to training/cp-0015.ckpt\n",
            "235/235 [==============================] - 24s 100ms/step - loss: 0.1722 - accuracy: 0.9349 - val_loss: 0.2258 - val_accuracy: 0.9201\n",
            "Epoch 16/200\n",
            "235/235 [==============================] - ETA: 0s - loss: 0.1678 - accuracy: 0.9385\n",
            "Epoch 16: saving model to training/cp-0016.ckpt\n",
            "235/235 [==============================] - 29s 122ms/step - loss: 0.1678 - accuracy: 0.9385 - val_loss: 0.2325 - val_accuracy: 0.9206\n",
            "Epoch 17/200\n",
            "235/235 [==============================] - ETA: 0s - loss: 0.1638 - accuracy: 0.9393\n",
            "Epoch 17: saving model to training/cp-0017.ckpt\n",
            "235/235 [==============================] - 23s 99ms/step - loss: 0.1638 - accuracy: 0.9393 - val_loss: 0.2295 - val_accuracy: 0.9207\n",
            "Epoch 18/200\n",
            "235/235 [==============================] - ETA: 0s - loss: 0.1621 - accuracy: 0.9392\n",
            "Epoch 18: saving model to training/cp-0018.ckpt\n",
            "235/235 [==============================] - 29s 124ms/step - loss: 0.1621 - accuracy: 0.9392 - val_loss: 0.2363 - val_accuracy: 0.9177\n",
            "Epoch 19/200\n",
            "235/235 [==============================] - ETA: 0s - loss: 0.1576 - accuracy: 0.9420\n",
            "Epoch 19: saving model to training/cp-0019.ckpt\n",
            "235/235 [==============================] - 24s 101ms/step - loss: 0.1576 - accuracy: 0.9420 - val_loss: 0.2293 - val_accuracy: 0.9229\n",
            "Epoch 20/200\n",
            "235/235 [==============================] - ETA: 0s - loss: 0.1508 - accuracy: 0.9433\n",
            "Epoch 20: saving model to training/cp-0020.ckpt\n",
            "235/235 [==============================] - 29s 124ms/step - loss: 0.1508 - accuracy: 0.9433 - val_loss: 0.2447 - val_accuracy: 0.9185\n",
            "Epoch 21/200\n",
            "235/235 [==============================] - ETA: 0s - loss: 0.1443 - accuracy: 0.9457\n",
            "Epoch 21: saving model to training/cp-0021.ckpt\n",
            "235/235 [==============================] - 24s 101ms/step - loss: 0.1443 - accuracy: 0.9457 - val_loss: 0.2391 - val_accuracy: 0.9213\n",
            "Epoch 22/200\n",
            "235/235 [==============================] - ETA: 0s - loss: 0.1428 - accuracy: 0.9467\n",
            "Epoch 22: saving model to training/cp-0022.ckpt\n",
            "235/235 [==============================] - 23s 100ms/step - loss: 0.1428 - accuracy: 0.9467 - val_loss: 0.2190 - val_accuracy: 0.9225\n",
            "Epoch 23/200\n",
            "235/235 [==============================] - ETA: 0s - loss: 0.1381 - accuracy: 0.9476\n",
            "Epoch 23: saving model to training/cp-0023.ckpt\n",
            "235/235 [==============================] - 29s 122ms/step - loss: 0.1381 - accuracy: 0.9476 - val_loss: 0.2274 - val_accuracy: 0.9223\n",
            "Epoch 24/200\n",
            "235/235 [==============================] - ETA: 0s - loss: 0.1335 - accuracy: 0.9498\n",
            "Epoch 24: saving model to training/cp-0024.ckpt\n",
            "235/235 [==============================] - 31s 130ms/step - loss: 0.1335 - accuracy: 0.9498 - val_loss: 0.2281 - val_accuracy: 0.9233\n",
            "Epoch 25/200\n",
            "235/235 [==============================] - ETA: 0s - loss: 0.1330 - accuracy: 0.9499\n",
            "Epoch 25: saving model to training/cp-0025.ckpt\n",
            "235/235 [==============================] - 27s 114ms/step - loss: 0.1330 - accuracy: 0.9499 - val_loss: 0.2382 - val_accuracy: 0.9223\n",
            "Epoch 26/200\n",
            "235/235 [==============================] - ETA: 0s - loss: 0.1263 - accuracy: 0.9529\n",
            "Epoch 26: saving model to training/cp-0026.ckpt\n",
            "235/235 [==============================] - 23s 99ms/step - loss: 0.1263 - accuracy: 0.9529 - val_loss: 0.2255 - val_accuracy: 0.9263\n",
            "Epoch 27/200\n",
            "235/235 [==============================] - ETA: 0s - loss: 0.1233 - accuracy: 0.9538\n",
            "Epoch 27: saving model to training/cp-0027.ckpt\n",
            "235/235 [==============================] - 24s 100ms/step - loss: 0.1233 - accuracy: 0.9538 - val_loss: 0.2385 - val_accuracy: 0.9239\n",
            "Epoch 28/200\n",
            "235/235 [==============================] - ETA: 0s - loss: 0.1173 - accuracy: 0.9553\n",
            "Epoch 28: saving model to training/cp-0028.ckpt\n",
            "235/235 [==============================] - 28s 117ms/step - loss: 0.1173 - accuracy: 0.9553 - val_loss: 0.2311 - val_accuracy: 0.9250\n",
            "Epoch 29/200\n",
            "235/235 [==============================] - ETA: 0s - loss: 0.1175 - accuracy: 0.9553\n",
            "Epoch 29: saving model to training/cp-0029.ckpt\n",
            "235/235 [==============================] - 29s 122ms/step - loss: 0.1175 - accuracy: 0.9553 - val_loss: 0.2461 - val_accuracy: 0.9224\n",
            "Epoch 30/200\n",
            "235/235 [==============================] - ETA: 0s - loss: 0.1170 - accuracy: 0.9562\n",
            "Epoch 30: saving model to training/cp-0030.ckpt\n",
            "235/235 [==============================] - 27s 117ms/step - loss: 0.1170 - accuracy: 0.9562 - val_loss: 0.2445 - val_accuracy: 0.9259\n",
            "Epoch 31/200\n",
            "235/235 [==============================] - ETA: 0s - loss: 0.1167 - accuracy: 0.9563\n",
            "Epoch 31: saving model to training/cp-0031.ckpt\n",
            "235/235 [==============================] - 26s 110ms/step - loss: 0.1167 - accuracy: 0.9563 - val_loss: 0.2753 - val_accuracy: 0.9211\n",
            "Epoch 32/200\n",
            "235/235 [==============================] - ETA: 0s - loss: 0.1129 - accuracy: 0.9579\n",
            "Epoch 32: saving model to training/cp-0032.ckpt\n",
            "235/235 [==============================] - 30s 126ms/step - loss: 0.1129 - accuracy: 0.9579 - val_loss: 0.2908 - val_accuracy: 0.9163\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 딥러닝 많은 데이터가 필요함. 성능에 비례\n",
        "# CNN 구조는 더 ㅁ낳은 파라미터들을 요구함\n",
        "# -> 더 많은 데이터가 필요한 부분이 따라옴\n",
        "# 1) 직접 수집의 양을 늘리거나\n",
        "# 2) 직접 내가 가상의 이미지를 생성해서 늘리거나,\n",
        "# 3)이미 잘 학습된 특징 추출에 대한 모델이 있어서, 그거 가져다가 사용을 함\n",
        "# (모델이 깊고, 학습 데이터도 방대함)\n",
        "# -> 나의 데이터 셋으로 Fine tuning 을 하는 방식(데이터 증강)\n",
        "# 이미지 데이터 셋 : hdd 보관을 할 수 없음!\n",
        "# -> 메모리에 모든 셋을 저장할 수 없음!\n",
        "# -> 메모리에 다 올라가면 빠르게 처리는 가능하나 불가능함\n",
        "\n",
        "# 이미지 저장은 나의 local/ HDD\n",
        "# 필요할 때마다 가지고 와서 모델에 토스(generator)\n",
        "# +batch_size(필수)\n",
        "# 좀 데이터를 증강할까 (옵션)\n",
        "# -> HDD - RAM 지연 발생, 학습 속도가 조금 느림\n",
        "# 본인 HW 상태를 모니터링 하면서 batch_size를 조절하실 필요 있음\n",
        "# 나중에 클라우드. 개인 워크스테이션에서 할 때 모델을 다 저장하면\n",
        "# 1개 모델의 용량이 크니, 나의 hdd 가 부족할 수 있음. 잘 관리 해야함\n",
        "# (클라우드: 클라우드가 멈춰 복구 불가능...\n",
        "#  미리 세팅ㅇ르 hdd 여유롭게 잡고 진행을 해야함)\n",
        "\n",
        "# DNN ---> CNN ----> VGG + 데이터 증강\n",
        "# 앞으로의 모델은 데이터 증강 사용안 하고,,모델 중심으로\n",
        "\n",
        "# 실제 kaggle 셋을 할 때에는 이미지 file + 폴더 구조 + 생성기(+ 증강)"
      ],
      "metadata": {
        "id": "hjLt0mzcn55n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}